{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gdown\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "####\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# \n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstr_loss(original_spectra, reconstructed_spectra, latent_dim):\n",
    "    '''Function to calculate reconstruction loss.\n",
    "\n",
    "    Inputs:\n",
    "    - original_spectra (np.array): original spectra.\n",
    "    - reconstructed_spectra (np.array): reconstruction of the original spectra from the latent representation.\n",
    "    - latent_dim (integer): size of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    - reconstruction loss with added penalty for the latent space size\n",
    "    '''\n",
    "\n",
    "    penalty = 0.00003\n",
    "    penalty2 = 5*0.00003\n",
    "\n",
    "\n",
    "    mse_loss = mean_squared_error(original_spectra, reconstructed_spectra, squared=True)\n",
    "    loss_penalized = mse_loss + latent_dim*penalty + penalty2*(latent_dim > 6)\n",
    "\n",
    "    return(loss_penalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = h5py.File( 'train_set.hdf5', 'r')\n",
    "x_train = np.array( data_train['spectra'] )\n",
    "\n",
    "data_val = h5py.File( 'val_set.hdf5', 'r')\n",
    "x_val = np.array( data_val['spectra'] )\n",
    "\n",
    "data_test = h5py.File('test_set.hdf5', 'r')\n",
    "x_test = np.array( data_test['spectra'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "x_train_sc = Scaler.fit_transform(x_train)\n",
    "x_val_sc   = Scaler.transform(x_val)\n",
    "x_test_sc  = Scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 39974)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstr_loss(original_spectra, reconstructed_spectra, latent_dim):\n",
    "    '''Function to calculate reconstruction loss.\n",
    "\n",
    "    Inputs:\n",
    "    - original_spectra (np.array): original spectra.\n",
    "    - reconstructed_spectra (np.array): reconstruction of the original spectra from the latent representation.\n",
    "    - latent_dim (integer): size of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    - reconstruction loss with added penalty for the latent space size\n",
    "    '''\n",
    "\n",
    "    penalty = 0.00003\n",
    "    penalty2 = 5*0.00003\n",
    "\n",
    "\n",
    "    mse_loss = mean_squared_error(original_spectra, reconstructed_spectra, squared=True)\n",
    "    loss_penalized = mse_loss + latent_dim*penalty + penalty2*(latent_dim > 6)\n",
    "\n",
    "    return(loss_penalized)\n",
    "\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "series_input = layers.Input(shape = (x_train.shape[1],1,))\n",
    "\n",
    "\n",
    "def make_encoder(hidden_nodes_list, activation_functions_list):\n",
    "    \n",
    "    \n",
    "\n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    encoder = models.Sequential()\n",
    "    encoder.add(series_input)\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            encoder.add(layers.Conv1D(n_nodes,16,activation=act, input_shape=[input_dim,1,]))\n",
    "        else: \n",
    "            encoder.add(layers.Conv1D(n_nodes,32,activation=act))\n",
    "\n",
    "        latent_dim = hidden_nodes_list[-1]\n",
    "        \n",
    "    return encoder , latent_dim\n",
    "\n",
    "def make_decoder(hidden_nodes_list, activation_functions_list, latent_dim):\n",
    "    latent_input = layers.Input(shape=(1,latent_dim,))\n",
    "    ## expanding \n",
    "    \n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    decoder = models.Sequential()\n",
    "    decoder.add(latent_input)\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            decoder.add(layers.Conv1DTranspose(n_nodes,32, activation=act, input_shape=(1,latent_dim,)))\n",
    "        else: \n",
    "            decoder.add(layers.Conv1DTranspose(n_nodes, 16,activation=act))\n",
    "#         decoder.add(layers.Conv1DTranspose(n_nodes, 1,activation='relu'))\n",
    "    return decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the convolutional layer\n",
    "\n",
    "Input shape\n",
    "\n",
    "3+D tensor with shape: batch_shape + (steps, input_dim)\n",
    "\n",
    "Output shape\n",
    "\n",
    "3+D tensor with shape: batch_shape + (new_steps, filters) steps value might have changed due to padding or strides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec1_net =models.Sequential()\n",
    "filters = 30 \n",
    "delay_size = 7\n",
    "# input_shape = (train_size,80)\n",
    "\n",
    "ec1_net.add(layers.Conv1D(filters ,  delay_size,activation='relu', input_shape = (x_train.shape[1],1,) ,padding=\"same\"))\n",
    "ec1_net.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "ec1_net.add(layers.Flatten())\n",
    "ec1_net.add(layers.Dense(128, activation = 'sigmoid'))\n",
    "ec1_net.add(layers.Flatten()) \n",
    "ec1_net.add(layers.Dense(64,activation='relu'))\n",
    "ec1_net.add(layers.Flatten()) \n",
    "ec1_net.add(layers.Dense(6,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(hidden_nodes_list, activation_functions_list, latent_dim):\n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    decoder = models.Sequential()\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            decoder.add(layers.Dense(n_nodes, activation=act, input_shape=(latent_dim,)))\n",
    "        else: \n",
    "            decoder.add(layers.Dense(n_nodes,activation=act))\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 6)                 76759094  \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 39974)             10628062  \n",
      "=================================================================\n",
      "Total params: 87,387,156\n",
      "Trainable params: 87,387,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "\n",
    "# hidden_nodes = [264,128,6]\n",
    "hidden_nodes_dec = [128,264,input_dim]\n",
    "\n",
    "# activation_e1= [ 'linear','relu','linear']#,'tanh','tanh','tanh']\n",
    "\n",
    "activation_d1= [ 'relu','relu','relu']#,'tanh','tanh','tanh']\n",
    "\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "epochs = 100 \n",
    "batch_size = 128\n",
    "\n",
    "# enc_act = ['sigmoid', 'tanh', 'linear','relu']\n",
    "# for act in enc_act:\n",
    "# act = 'relu'\n",
    "# print(f'training for activation {act}')\n",
    "# activation_e1[1] = act\n",
    "# encoder,ld = make_encoder(hidden_nodes, activation_e1)\n",
    "ld = 6\n",
    "dc1_net = make_decoder(hidden_nodes_dec,activation_d1, latent_dim = ld)\n",
    "network=models.Sequential()\n",
    "network.add(ec1_net)\n",
    "network.add(dc1_net)\n",
    "network.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 39974, 30)         240       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 19987, 30)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 599610)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               76750208  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 76,759,094\n",
      "Trainable params: 76,759,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ec1_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 264)               34056     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 39974)             10593110  \n",
      "=================================================================\n",
      "Total params: 10,628,062\n",
      "Trainable params: 10,628,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dc1_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 39974)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 09:08:47.003235: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "51/51 [==============================] - 43s 830ms/step - loss: 0.9963 - mse: 0.9963 - val_loss: 1.0252 - val_mse: 1.0252\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 41s 802ms/step - loss: 0.9956 - mse: 0.9956 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 41s 803ms/step - loss: 0.9956 - mse: 0.9956 - val_loss: 1.0250 - val_mse: 1.0250\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 41s 806ms/step - loss: 0.9954 - mse: 0.9954 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 43s 840ms/step - loss: 0.9950 - mse: 0.9950 - val_loss: 1.0244 - val_mse: 1.0244\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 44s 869ms/step - loss: 0.9947 - mse: 0.9947 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 43s 834ms/step - loss: 0.9944 - mse: 0.9944 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 41s 805ms/step - loss: 0.9940 - mse: 0.9940 - val_loss: 1.0234 - val_mse: 1.0234\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 44s 867ms/step - loss: 0.9937 - mse: 0.9937 - val_loss: 1.0232 - val_mse: 1.0232\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 55s 1s/step - loss: 0.9935 - mse: 0.9935 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 53s 1s/step - loss: 0.9934 - mse: 0.9934 - val_loss: 1.0229 - val_mse: 1.0229\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 49s 952ms/step - loss: 0.9933 - mse: 0.9933 - val_loss: 1.0228 - val_mse: 1.0228\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 44s 865ms/step - loss: 0.9932 - mse: 0.9932 - val_loss: 1.0228 - val_mse: 1.0228\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 43s 852ms/step - loss: 0.9931 - mse: 0.9931 - val_loss: 1.0227 - val_mse: 1.0227\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 44s 861ms/step - loss: 0.9930 - mse: 0.9930 - val_loss: 1.0226 - val_mse: 1.0226\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 44s 864ms/step - loss: 0.9929 - mse: 0.9929 - val_loss: 1.0225 - val_mse: 1.0225\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 44s 864ms/step - loss: 0.9929 - mse: 0.9929 - val_loss: 1.0225 - val_mse: 1.0225\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 47s 915ms/step - loss: 0.9928 - mse: 0.9928 - val_loss: 1.0224 - val_mse: 1.0224\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 45s 891ms/step - loss: 0.9927 - mse: 0.9927 - val_loss: 1.0223 - val_mse: 1.0223\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 42s 823ms/step - loss: 0.9926 - mse: 0.9926 - val_loss: 1.0222 - val_mse: 1.0222\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 45s 882ms/step - loss: 0.9925 - mse: 0.9925 - val_loss: 1.0221 - val_mse: 1.0221\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 44s 872ms/step - loss: 0.9924 - mse: 0.9924 - val_loss: 1.0221 - val_mse: 1.0221\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.9923 - mse: 0.9923 - val_loss: 1.0220 - val_mse: 1.0220\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 53s 1s/step - loss: 0.9923 - mse: 0.9923 - val_loss: 1.0219 - val_mse: 1.0219\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 48s 943ms/step - loss: 0.9922 - mse: 0.9922 - val_loss: 1.0219 - val_mse: 1.0219\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 43s 852ms/step - loss: 0.9921 - mse: 0.9921 - val_loss: 1.0218 - val_mse: 1.0218\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 44s 855ms/step - loss: 0.9920 - mse: 0.9920 - val_loss: 1.0217 - val_mse: 1.0217\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 43s 851ms/step - loss: 0.9920 - mse: 0.9920 - val_loss: 1.0217 - val_mse: 1.0217\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 43s 851ms/step - loss: 0.9919 - mse: 0.9919 - val_loss: 1.0217 - val_mse: 1.0217\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 43s 853ms/step - loss: 0.9918 - mse: 0.9918 - val_loss: 1.0215 - val_mse: 1.0215\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 45s 881ms/step - loss: 0.9917 - mse: 0.9917 - val_loss: 1.0215 - val_mse: 1.0215\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 45s 883ms/step - loss: 0.9916 - mse: 0.9916 - val_loss: 1.0214 - val_mse: 1.0214\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 41s 813ms/step - loss: 0.9915 - mse: 0.9915 - val_loss: 1.0213 - val_mse: 1.0213\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 48s 938ms/step - loss: 0.9915 - mse: 0.9915 - val_loss: 1.0213 - val_mse: 1.0213\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 57s 1s/step - loss: 0.9914 - mse: 0.9914 - val_loss: 1.0212 - val_mse: 1.0212\n",
      "Epoch 36/100\n",
      "51/51 [==============================] - 62s 1s/step - loss: 0.9913 - mse: 0.9913 - val_loss: 1.0211 - val_mse: 1.0211\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 60s 1s/step - loss: 0.9912 - mse: 0.9912 - val_loss: 1.0211 - val_mse: 1.0211\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 55s 1s/step - loss: 0.9912 - mse: 0.9912 - val_loss: 1.0210 - val_mse: 1.0210\n",
      "Epoch 39/100\n",
      "51/51 [==============================] - 52s 1s/step - loss: 0.9911 - mse: 0.9911 - val_loss: 1.0210 - val_mse: 1.0210\n",
      "Epoch 40/100\n",
      "51/51 [==============================] - 53s 1s/step - loss: 0.9910 - mse: 0.9910 - val_loss: 1.0209 - val_mse: 1.0209\n",
      "Epoch 41/100\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.9910 - mse: 0.9910 - val_loss: 1.0209 - val_mse: 1.0209\n",
      "Epoch 42/100\n",
      "51/51 [==============================] - 52s 1s/step - loss: 0.9909 - mse: 0.9909 - val_loss: 1.0208 - val_mse: 1.0208\n",
      "Epoch 43/100\n",
      "51/51 [==============================] - 55s 1s/step - loss: 0.9909 - mse: 0.9909 - val_loss: 1.0208 - val_mse: 1.0208\n",
      "Epoch 44/100\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.9908 - mse: 0.9908 - val_loss: 1.0208 - val_mse: 1.0208\n",
      "Epoch 45/100\n",
      "51/51 [==============================] - 53s 1s/step - loss: 0.9908 - mse: 0.9908 - val_loss: 1.0208 - val_mse: 1.0208\n",
      "Epoch 46/100\n",
      "51/51 [==============================] - 47s 926ms/step - loss: 0.9907 - mse: 0.9907 - val_loss: 1.0207 - val_mse: 1.0207\n",
      "Epoch 47/100\n",
      "51/51 [==============================] - 46s 902ms/step - loss: 0.9907 - mse: 0.9907 - val_loss: 1.0207 - val_mse: 1.0207\n",
      "Epoch 48/100\n",
      "51/51 [==============================] - 46s 905ms/step - loss: 0.9906 - mse: 0.9906 - val_loss: 1.0207 - val_mse: 1.0207\n",
      "Epoch 49/100\n",
      "51/51 [==============================] - 46s 895ms/step - loss: 0.9906 - mse: 0.9906 - val_loss: 1.0206 - val_mse: 1.0206\n",
      "Epoch 50/100\n",
      "51/51 [==============================] - 47s 917ms/step - loss: 0.9906 - mse: 0.9906 - val_loss: 1.0206 - val_mse: 1.0206\n",
      "Epoch 51/100\n",
      "51/51 [==============================] - 46s 900ms/step - loss: 0.9905 - mse: 0.9905 - val_loss: 1.0206 - val_mse: 1.0206\n",
      "Epoch 52/100\n",
      "51/51 [==============================] - 47s 916ms/step - loss: 0.9905 - mse: 0.9905 - val_loss: 1.0206 - val_mse: 1.0206\n",
      "Epoch 53/100\n",
      "51/51 [==============================] - 45s 887ms/step - loss: 0.9905 - mse: 0.9905 - val_loss: 1.0206 - val_mse: 1.0206\n",
      "Epoch 54/100\n",
      "51/51 [==============================] - 47s 916ms/step - loss: 0.9904 - mse: 0.9904 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 55/100\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.9904 - mse: 0.9904 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 56/100\n",
      "51/51 [==============================] - 53s 1s/step - loss: 0.9904 - mse: 0.9904 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 57/100\n",
      "51/51 [==============================] - 47s 903ms/step - loss: 0.9903 - mse: 0.9903 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 58/100\n",
      "51/51 [==============================] - 43s 848ms/step - loss: 0.9903 - mse: 0.9903 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 59/100\n",
      "51/51 [==============================] - 43s 843ms/step - loss: 0.9903 - mse: 0.9903 - val_loss: 1.0204 - val_mse: 1.0204\n",
      "Epoch 60/100\n",
      "51/51 [==============================] - 43s 846ms/step - loss: 0.9902 - mse: 0.9902 - val_loss: 1.0204 - val_mse: 1.0204\n",
      "Epoch 61/100\n",
      "51/51 [==============================] - 44s 857ms/step - loss: 0.9902 - mse: 0.9902 - val_loss: 1.0204 - val_mse: 1.0204\n",
      "Epoch 62/100\n",
      "51/51 [==============================] - 44s 867ms/step - loss: 0.9902 - mse: 0.9902 - val_loss: 1.0204 - val_mse: 1.0204\n",
      "Epoch 63/100\n",
      "51/51 [==============================] - 47s 922ms/step - loss: 0.9901 - mse: 0.9901 - val_loss: 1.0204 - val_mse: 1.0204\n",
      "Epoch 64/100\n",
      "51/51 [==============================] - 45s 880ms/step - loss: 0.9901 - mse: 0.9901 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 65/100\n",
      "51/51 [==============================] - 43s 835ms/step - loss: 0.9901 - mse: 0.9901 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 66/100\n",
      "51/51 [==============================] - 45s 877ms/step - loss: 0.9900 - mse: 0.9900 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 67/100\n",
      "51/51 [==============================] - 57s 1s/step - loss: 0.9900 - mse: 0.9900 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 68/100\n",
      "51/51 [==============================] - 55s 1s/step - loss: 0.9900 - mse: 0.9900 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "Epoch 69/100\n",
      "51/51 [==============================] - 48s 945ms/step - loss: 0.9899 - mse: 0.9899 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "Epoch 70/100\n",
      "51/51 [==============================] - 45s 873ms/step - loss: 0.9899 - mse: 0.9899 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "Epoch 71/100\n",
      "51/51 [==============================] - 45s 884ms/step - loss: 0.9899 - mse: 0.9899 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "Epoch 72/100\n",
      "51/51 [==============================] - 44s 867ms/step - loss: 0.9898 - mse: 0.9898 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 73/100\n",
      "51/51 [==============================] - 51s 994ms/step - loss: 0.9898 - mse: 0.9898 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "Epoch 74/100\n",
      "51/51 [==============================] - 42s 820ms/step - loss: 0.9898 - mse: 0.9898 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 75/100\n",
      "51/51 [==============================] - 43s 845ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 76/100\n",
      "51/51 [==============================] - 43s 853ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 77/100\n",
      "51/51 [==============================] - 49s 967ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 78/100\n",
      "51/51 [==============================] - 55s 1s/step - loss: 0.9897 - mse: 0.9897 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 79/100\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.9896 - mse: 0.9896 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 80/100\n",
      "51/51 [==============================] - 53s 1s/step - loss: 0.9896 - mse: 0.9896 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 81/100\n",
      "51/51 [==============================] - 46s 894ms/step - loss: 0.9896 - mse: 0.9896 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 82/100\n",
      "51/51 [==============================] - 47s 915ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 83/100\n",
      "51/51 [==============================] - 46s 907ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 84/100\n",
      "51/51 [==============================] - 46s 899ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 85/100\n",
      "51/51 [==============================] - 42s 828ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 86/100\n",
      "51/51 [==============================] - 58s 1s/step - loss: 0.9894 - mse: 0.9894 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 87/100\n",
      "51/51 [==============================] - 59s 1s/step - loss: 0.9894 - mse: 0.9894 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 88/100\n",
      "51/51 [==============================] - 57s 1s/step - loss: 0.9894 - mse: 0.9894 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 89/100\n",
      "51/51 [==============================] - 51s 1s/step - loss: 0.9894 - mse: 0.9894 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 90/100\n",
      "51/51 [==============================] - 51s 1s/step - loss: 0.9893 - mse: 0.9893 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 91/100\n",
      "51/51 [==============================] - 52s 1s/step - loss: 0.9893 - mse: 0.9893 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 92/100\n",
      "51/51 [==============================] - 50s 976ms/step - loss: 0.9893 - mse: 0.9893 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 93/100\n",
      "51/51 [==============================] - 45s 893ms/step - loss: 0.9893 - mse: 0.9893 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 94/100\n",
      "51/51 [==============================] - 50s 973ms/step - loss: 0.9893 - mse: 0.9893 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 95/100\n",
      "51/51 [==============================] - 48s 939ms/step - loss: 0.9892 - mse: 0.9892 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 96/100\n",
      "51/51 [==============================] - 45s 890ms/step - loss: 0.9892 - mse: 0.9892 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 97/100\n",
      "51/51 [==============================] - 46s 897ms/step - loss: 0.9892 - mse: 0.9892 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 98/100\n",
      "51/51 [==============================] - 46s 910ms/step - loss: 0.9891 - mse: 0.9891 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 99/100\n",
      "51/51 [==============================] - 46s 892ms/step - loss: 0.9891 - mse: 0.9891 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 100/100\n",
      "51/51 [==============================] - 47s 932ms/step - loss: 0.9891 - mse: 0.9891 - val_loss: 1.0198 - val_mse: 1.0198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_resh = np.reshape(x_train_sc, (x_train_sc.shape[0], x_train_sc.shape[1], 1))\n",
    "\n",
    "x_val_resh = np.reshape(x_val_sc, (x_val_sc.shape[0], x_val_sc.shape[1], 1))\n",
    "\n",
    "\n",
    "history = network.fit(x_train_resh,x_train_resh,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=[earlystopping], # Early stopping\n",
    "              validation_data=(x_val_resh,x_val_resh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0060450752819842845\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### train the network \n",
    "\n",
    "\n",
    "# weather_input_train = np.expand_dims(weather_input_train, axis=1)\n",
    "\n",
    "x_test_resh = np.reshape(x_test_sc, (x_test_sc.shape[0], x_test_sc.shape[1], 1))\n",
    "\n",
    "\n",
    "\n",
    "#     network.save(filepath)\n",
    "\n",
    "## check reconstruction loss \n",
    "test_set_latent_encoding = ec1_net(x_test_resh)\n",
    "test_set_reconstructions = dc1_net(test_set_latent_encoding)\n",
    "    # \n",
    "x_test_reconstructed = Scaler.inverse_transform(test_set_reconstructions)\n",
    "x_test_reconstructed = np.reshape(x_test_reconstructed, (x_test_reconstructed.shape[0], x_test_reconstructed.shape[1] ))\n",
    "\n",
    "rl = reconstr_loss(x_test, x_test_reconstructed, latent_dim = test_set_latent_encoding.shape[1])\n",
    "print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
