{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gdown\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "####\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstr_loss(original_spectra, reconstructed_spectra, latent_dim):\n",
    "    '''Function to calculate reconstruction loss.\n",
    "\n",
    "    Inputs:\n",
    "    - original_spectra (np.array): original spectra.\n",
    "    - reconstructed_spectra (np.array): reconstruction of the original spectra from the latent representation.\n",
    "    - latent_dim (integer): size of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    - reconstruction loss with added penalty for the latent space size\n",
    "    '''\n",
    "\n",
    "    penalty = 0.00003\n",
    "    penalty2 = 5*0.00003\n",
    "\n",
    "\n",
    "    mse_loss = mean_squared_error(original_spectra, reconstructed_spectra, squared=True)\n",
    "    loss_penalized = mse_loss + latent_dim*penalty + penalty2*(latent_dim > 6)\n",
    "\n",
    "    return(loss_penalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/blue/pdixit/hodaakl/Hackathon2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = h5py.File( 'train_set.hdf5', 'r')\n",
    "x_train = np.array( data_train['spectra'] )\n",
    "\n",
    "data_val = h5py.File( 'val_set.hdf5', 'r')\n",
    "x_val = np.array( data_val['spectra'] )\n",
    "\n",
    "data_test = h5py.File('test_set.hdf5', 'r')\n",
    "x_test = np.array( data_test['spectra'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess the data \n",
    "### denoise the data \n",
    "x_train_den = x_train.copy()\n",
    "x_val_den = x_val.copy()\n",
    "x_test_den = x_test.copy()\n",
    "\n",
    "xidx, yidx = np.where(x_train>.9)\n",
    "x_train_den[xidx, yidx] = np.ones(len(xidx))\n",
    "xidx, yidx = np.where(x_val>.9)\n",
    "x_val_den[xidx, yidx] = np.ones(len(xidx))\n",
    "xidx, yidx = np.where(x_test>.9)\n",
    "x_test_den[xidx, yidx] = np.ones(len(xidx))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "x_at_n11_train = Scaler.fit_transform(x_train_den)\n",
    "x_at_n11_val = Scaler.transform(x_val_den)\n",
    "x_at_n11_test = Scaler.transform(x_test_den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 16:54:13.126645: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-30 16:54:13.126702: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-30 16:54:13.126740: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c0706a-s17.ufhpc): /proc/driver/nvidia/version does not exist\n",
      "2021-11-30 16:54:13.127089: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mod = load_model('StandardScalerModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_6 (Sequential)    (None, 6)                 10588094  \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 39974)             10628062  \n",
      "=================================================================\n",
      "Total params: 21,216,156\n",
      "Trainable params: 21,216,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = mod.get_layer(index=0)\n",
    "decoder = mod.get_layer(index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 264)               10553400  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               33920     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 10,588,094\n",
      "Trainable params: 10,588,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 264)               34056     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 39974)             10593110  \n",
      "=================================================================\n",
      "Total params: 10,628,062\n",
      "Trainable params: 10,628,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0022086372084942126\n"
     ]
    }
   ],
   "source": [
    "test_set_latent_encoding = encoder(x_at_n11_test)\n",
    "test_set_reconstructions = decoder(test_set_latent_encoding)\n",
    "    # \n",
    "from pickle import load\n",
    "Scaler2 = load(open('StandardScaler.pkl', 'rb'))\n",
    "x_test_reconstructed = Scaler2.inverse_transform(test_set_reconstructions)\n",
    "# x_test_reconstructed = np.reshape(x_test_reconstructed, (x_test_reconstructed.shape[0], x_test_reconstructed.shape[1] ))\n",
    "\n",
    "rl = reconstr_loss(x_test, x_test_reconstructed, latent_dim = test_set_latent_encoding.shape[1])\n",
    "print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
