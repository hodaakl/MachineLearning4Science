{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Hoda Akl <br> \n",
    "\n",
    "Note: Preprocessing the data is key to my result. I scale the data before training, the test data will also be scaled the same way. In order to do the scaling, the x_train data should be loaded in this notebook, there is also an option of loading the scaler. \n",
    "\n",
    "I have included in the email the model file, please put it in the same directory as this notebook.\n",
    "\n",
    "Below you will find \n",
    "\n",
    "Part 1: The Model architecture\n",
    "\n",
    "Part 2: Two functions with loaded weights to be used to the evaluation metric. \n",
    "\n",
    "Part 3: The reconstruction error on the test data set (0.0022086372084942126)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "Notes on my scaling : \n",
    "\n",
    "1. first I denoise the data by imposing the threshold of 0.9 above which I consider all values to be one \n",
    "2. Second, on the denoised data I import StandardScaler from Sklearn preprosessing. \n",
    "\n",
    "To keep this notebook concise I do not do not define the scaler here from sklearn but I load it. The file is provided in the email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages and defining the reconstruction function \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "####\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import load_model\n",
    "#\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "def reconstr_loss(original_spectra, reconstructed_spectra, latent_dim):\n",
    "    '''Function to calculate reconstruction loss.\n",
    "\n",
    "    Inputs:\n",
    "    - original_spectra (np.array): original spectra.\n",
    "    - reconstructed_spectra (np.array): reconstruction of the original spectra from the latent representation.\n",
    "    - latent_dim (integer): size of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    - reconstruction loss with added penalty for the latent space size\n",
    "    '''\n",
    "\n",
    "    penalty = 0.00003\n",
    "    penalty2 = 5*0.00003\n",
    "\n",
    "\n",
    "    mse_loss = mean_squared_error(original_spectra, reconstructed_spectra, squared=True)\n",
    "    loss_penalized = mse_loss + latent_dim*penalty + penalty2*(latent_dim > 6)\n",
    "\n",
    "    return(loss_penalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data \n",
    "\n",
    "# load the data to define some dimensions\n",
    "\n",
    "# download if data is not downloaded\n",
    "# url_train = 'https://drive.google.com/u/0/uc?export=download&confirm=F_-k&id=1sdx-m9PLLKjPQ8J2g7H2zw2FUSMV6jYz'\n",
    "# url_val = 'https://drive.google.com/u/0/uc?export=download&confirm=QL45&id=1qymhB00l4wy_Ql4A3DRxguxytFzda7_0'\n",
    "# url_test = 'https://drive.google.com/u/0/uc?export=download&confirm=nAB1&id=1lhJl_6lWCxNpOSxJd9_d_qssgUKEli7M'\n",
    "\n",
    "# # Using this links may be faster (but can fail if many users are connected)\n",
    "# # url_train = 'https://drive.google.com/uc?id=1sdx-m9PLLKjPQ8J2g7H2zw2FUSMV6jYz'\n",
    "# # url_val = 'https://drive.google.com/uc?id=1qymhB00l4wy_Ql4A3DRxguxytFzda7_0'\n",
    "# # url_test = 'https://drive.google.com/uc?id=1lhJl_6lWCxNpOSxJd9_d_qssgUKEli7M'\n",
    "\n",
    "\n",
    "# data_train = gdown.download(url_train, 'train_set.hdf5', quiet=False)\n",
    "# data_val = gdown.download(url_val, 'val_set.hdf5', quiet=False)\n",
    "# data_test = gdown.download(url_test, 'test_set.hdf5', quiet=False)\n",
    "\n",
    "data_train = h5py.File( 'train_set.hdf5', 'r')\n",
    "x_train = np.array( data_train['spectra'] )\n",
    "\n",
    "data_val = h5py.File( 'val_set.hdf5', 'r')\n",
    "x_val = np.array( data_val['spectra'] )\n",
    "\n",
    "data_test = h5py.File('test_set.hdf5', 'r')\n",
    "x_test = np.array( data_test['spectra'] )\n",
    "\n",
    "### preprocess and define the Scaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "x_Train_den = x_train.copy()\n",
    "xidx, yidx = np.where(x_Train_den>.9)\n",
    "x_Train_den[xidx, yidx] = np.ones(len(xidx))\n",
    "Scaler = StandardScaler()\n",
    "x_data_sc = Scaler.fit_transform(x_Train_den)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: The model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model functions \n",
    "def make_encoder(hidden_nodes_list, activation_functions_list):\n",
    "\n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    encoder = models.Sequential()\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            encoder.add(layers.Dense(n_nodes,activation=act, input_shape=(input_dim,)))\n",
    "        else: \n",
    "            encoder.add(layers.Dense(n_nodes,activation=act))\n",
    "\n",
    "        latent_dim = hidden_nodes_list[-1]\n",
    "    return encoder , latent_dim\n",
    "\n",
    "def make_decoder(hidden_nodes_list, activation_functions_list, latent_dim):\n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    decoder = models.Sequential()\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            decoder.add(layers.Dense(n_nodes, activation=act, input_shape=(latent_dim,)))\n",
    "        else: \n",
    "            decoder.add(layers.Dense(n_nodes,activation=act))\n",
    "    return decoder \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to run the fitting, run the following two cells. Make sure to have the file StandardScaler.pkl in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pickle import load\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Scaler = StandardScaler()\n",
    "# x_Train_den = x_train.copy()\n",
    "# xidx, yidx = np.where(x_Train_den>.9)\n",
    "# x_Train_den[xidx, yidx] = np.ones(len(xidx))\n",
    "# Scaler = StandardScaler()\n",
    "# x_data_sc = Scaler.fit_transform(x_Train_den)\n",
    "def Preprocess_fn(data, Scaler = Scaler ): \n",
    "    # denoise the data \n",
    "    data_den = data.copy()\n",
    "    xidx, yidx = np.where(data_den>.9)\n",
    "    data_den[xidx, yidx] = np.ones(len(xidx))\n",
    "    data_trans = Scaler.transform(data_den)\n",
    "    \n",
    "    return data_trans\n",
    "\n",
    "def DeProcess_fn(recons, Scaler = Scaler ):\n",
    "\n",
    "\n",
    "    Reconstructions = Scaler.inverse_transform(recons)\n",
    "    \n",
    "    return Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 6)                 10588094  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 39974)             10628062  \n",
      "=================================================================\n",
      "Total params: 21,216,156\n",
      "Trainable params: 21,216,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 18:28:00.020674: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-30 18:28:00.020725: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-30 18:28:00.020756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c23b-s36.ufhpc): /proc/driver/nvidia/version does not exist\n",
      "2021-11-30 18:28:00.021201: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "\n",
    "hidden_nodes = [264,128,6]\n",
    "hidden_nodes_dec = [128,264,input_dim]\n",
    "\n",
    "activation_e1= [ 'linear','relu','linear']#,'tanh','tanh','tanh']\n",
    "\n",
    "activation_d1= [ 'relu','relu','linear']#,'tanh','tanh','tanh']\n",
    "\n",
    "model_path = 'mcheckp_2.h5'\n",
    "encoder,ld = make_encoder(hidden_nodes, activation_e1)\n",
    "decoder = make_decoder(hidden_nodes_dec,activation_d1, latent_dim = ld)\n",
    "network=models.Sequential()\n",
    "network.add(encoder)\n",
    "network.add(decoder)\n",
    "network.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "network.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 18:28:14.919957: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "51/51 [==============================] - 4s 64ms/step - loss: 0.8740 - mse: 0.8740 - val_loss: 1.0132 - val_mse: 1.0132\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 3s 50ms/step - loss: 0.7952 - mse: 0.7952 - val_loss: 0.9888 - val_mse: 0.9888\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 3s 59ms/step - loss: 0.7552 - mse: 0.7552 - val_loss: 0.9758 - val_mse: 0.9758\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 3s 61ms/step - loss: 0.7669 - mse: 0.7669 - val_loss: 0.9525 - val_mse: 0.9525\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 3s 55ms/step - loss: 0.7620 - mse: 0.7620 - val_loss: 0.9572 - val_mse: 0.9572\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 3s 55ms/step - loss: 0.7453 - mse: 0.7453 - val_loss: 0.9524 - val_mse: 0.9524\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.7538 - mse: 0.7538 - val_loss: 0.9486 - val_mse: 0.9486\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.7980 - mse: 0.7980 - val_loss: 0.9633 - val_mse: 0.9633\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 1.0172 - mse: 1.0172 - val_loss: 0.9557 - val_mse: 0.9557\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 3s 56ms/step - loss: 0.9485 - mse: 0.9485 - val_loss: 0.9955 - val_mse: 0.9955\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 3s 55ms/step - loss: 0.7720 - mse: 0.7720 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.7240 - mse: 0.7240 - val_loss: 0.9397 - val_mse: 0.9397\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 3s 56ms/step - loss: 0.7161 - mse: 0.7161 - val_loss: 0.9359 - val_mse: 0.9359\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.7491 - mse: 0.7491 - val_loss: 0.9495 - val_mse: 0.9495\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.7532 - mse: 0.7532 - val_loss: 0.9355 - val_mse: 0.9355\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.7166 - mse: 0.7166 - val_loss: 0.9351 - val_mse: 0.9351\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6917 - mse: 0.6917 - val_loss: 0.9169 - val_mse: 0.9169\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6859 - mse: 0.6859 - val_loss: 0.9119 - val_mse: 0.9119\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 3s 55ms/step - loss: 0.6769 - mse: 0.6769 - val_loss: 0.9081 - val_mse: 0.9081\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 3s 56ms/step - loss: 0.6754 - mse: 0.6754 - val_loss: 0.9072 - val_mse: 0.9072\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 3s 60ms/step - loss: 0.6698 - mse: 0.6698 - val_loss: 0.9121 - val_mse: 0.9121\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6655 - mse: 0.6655 - val_loss: 0.9087 - val_mse: 0.9087\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 3s 55ms/step - loss: 0.6570 - mse: 0.6570 - val_loss: 0.9120 - val_mse: 0.9120\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.6568 - mse: 0.6568 - val_loss: 0.9069 - val_mse: 0.9069\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 3s 56ms/step - loss: 0.6570 - mse: 0.6570 - val_loss: 0.9076 - val_mse: 0.9076\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.6577 - mse: 0.6577 - val_loss: 0.9118 - val_mse: 0.9118\n",
      "Epoch 27/100\n",
      "51/51 [==============================] - 3s 60ms/step - loss: 0.6576 - mse: 0.6576 - val_loss: 0.9039 - val_mse: 0.9039\n",
      "Epoch 28/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6502 - mse: 0.6502 - val_loss: 0.8996 - val_mse: 0.8996\n",
      "Epoch 29/100\n",
      "51/51 [==============================] - 3s 55ms/step - loss: 0.6457 - mse: 0.6457 - val_loss: 0.9068 - val_mse: 0.9068\n",
      "Epoch 30/100\n",
      "51/51 [==============================] - 3s 61ms/step - loss: 0.6542 - mse: 0.6542 - val_loss: 0.9130 - val_mse: 0.9130\n",
      "Epoch 31/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.6585 - mse: 0.6585 - val_loss: 0.9106 - val_mse: 0.9106\n",
      "Epoch 32/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6717 - mse: 0.6717 - val_loss: 0.9199 - val_mse: 0.9199\n",
      "Epoch 33/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.6816 - mse: 0.6816 - val_loss: 0.9195 - val_mse: 0.9195\n",
      "Epoch 34/100\n",
      "51/51 [==============================] - 3s 60ms/step - loss: 0.6646 - mse: 0.6646 - val_loss: 0.9149 - val_mse: 0.9149\n",
      "Epoch 35/100\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.6616 - mse: 0.6616 - val_loss: 0.9151 - val_mse: 0.9151\n",
      "Epoch 36/100\n",
      "51/51 [==============================] - 3s 59ms/step - loss: 0.7009 - mse: 0.7009 - val_loss: 0.9020 - val_mse: 0.9020\n",
      "Epoch 37/100\n",
      "51/51 [==============================] - 3s 59ms/step - loss: 0.6692 - mse: 0.6692 - val_loss: 0.9154 - val_mse: 0.9154\n",
      "Epoch 38/100\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6984 - mse: 0.6984 - val_loss: 0.9095 - val_mse: 0.9095\n"
     ]
    }
   ],
   "source": [
    "# uncomment the following line if you wish to run \n",
    "x_tr_pp = Preprocess_fn(x_train)\n",
    "x_val_pp = Preprocess_fn(x_val)\n",
    "keras_callbacks   = [\n",
    "      EarlyStopping(monitor='val_loss', patience=10),\n",
    "      ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "### UNCOMMENT THE FOLLOWING LINE FOR FITTING\n",
    "epochs = 100 \n",
    "batch_size = 128 \n",
    "\n",
    "\n",
    "history = network.fit(x_tr_pp,x_tr_pp,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=keras_callbacks, # Early stopping\n",
    "              validation_data=(x_val_pp,x_val_pp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Functions to get to the latents and the reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those functions use the scaler file and the model file accompanied on the same email. \n",
    "Please download them to the same directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess function \n",
    "import os\n",
    "\n",
    "# if you can not load the scaler here is how to define it \n",
    "### UNcomment the following lines \n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Scaler = StandardScaler()\n",
    "# x_Train_den = x_train.copy()\n",
    "# xidx, yidx = np.where(x_Train_den>.9)\n",
    "# x_Train_den[xidx, yidx] = np.ones(len(xidx))\n",
    "# Scaler = StandardScaler()\n",
    "# x_data_sc = Scaler.fit_transform(x_Train_den)\n",
    "\n",
    "\n",
    "\n",
    "# from pickle import load\n",
    "def Preprocess_fn(data, Scaler = Scaler ): \n",
    "    # denoise the data \n",
    "    data_den = data.copy()\n",
    "    xidx, yidx = np.where(data_den>.9)\n",
    "    data_den[xidx, yidx] = np.ones(len(xidx))\n",
    "    data_trans = Scaler.transform(data_den)\n",
    "    \n",
    "    return data_trans\n",
    "\n",
    "def DeProcess_fn(recons, Scaler = Scaler ):\n",
    "\n",
    "\n",
    "    Reconstructions = Scaler.inverse_transform(recons)\n",
    "    \n",
    "    return Reconstructions\n",
    "    \n",
    "\n",
    "def reduce_dimensionality(data, model_path = model_path): \n",
    "    \n",
    "    data_pp = Preprocess_fn(data)\n",
    "    ## load the encoder from the saved model \n",
    "    if os.path.exists(model_path) != True: \n",
    "        raise ValueError(\"Model path does not exists, please make sure it is in the right place\") \n",
    "    mod = load_model(model_path)\n",
    "    encoder = mod.get_layer(index=0)\n",
    "    ## get latents\n",
    "    test_set_latent_encoding = encoder(data_pp)\n",
    "    return test_set_latent_encoding\n",
    "    \n",
    "    \n",
    "\n",
    "def calculate_reconstructions(latents, model_path = model_path): \n",
    "    if os.path.exists(model_path) != True: \n",
    "        raise ValueError(\"Model path does not exists, please make sure it is in the right place\") \n",
    "    mod = load_model(model_path)\n",
    "    decoder = mod.get_layer(index=1)\n",
    "    decodings = decoder(latents)\n",
    "    # revert back - inverse scaling \n",
    "    recons = DeProcess_fn(decodings)\n",
    "    \n",
    "    return recons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Test set reconstruction loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = reduce_dimensionality(x_test, model_path = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_reconstructions = calculate_reconstructions(lats, model_path = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0022919134355735547"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstr_loss(x_test, test_set_reconstructions, latent_dim = lats.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
