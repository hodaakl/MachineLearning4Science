{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through playing with scaling of the data in notebook CGM_Scaling.ipynb I found that standard scaling works best. Here I will try and optimize network architecture and figure out why standard scaling would work best <br> \n",
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gdown\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "####\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = h5py.File( 'train_set.hdf5', 'r')\n",
    "x_train = np.array( data_train['spectra'] )\n",
    "\n",
    "data_val = h5py.File( 'val_set.hdf5', 'r')\n",
    "x_val = np.array( data_val['spectra'] )\n",
    "\n",
    "data_test = h5py.File('test_set.hdf5', 'r')\n",
    "x_test = np.array( data_test['spectra'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstr_loss(original_spectra, reconstructed_spectra, latent_dim):\n",
    "    '''Function to calculate reconstruction loss.\n",
    "\n",
    "    Inputs:\n",
    "    - original_spectra (np.array): original spectra.\n",
    "    - reconstructed_spectra (np.array): reconstruction of the original spectra from the latent representation.\n",
    "    - latent_dim (integer): size of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    - reconstruction loss with added penalty for the latent space size\n",
    "    '''\n",
    "\n",
    "    penalty = 0.00003\n",
    "    penalty2 = 5*0.00003\n",
    "\n",
    "\n",
    "    mse_loss = mean_squared_error(original_spectra, reconstructed_spectra, squared=True)\n",
    "    loss_penalized = mse_loss + latent_dim*penalty + penalty2*(latent_dim > 6)\n",
    "\n",
    "    return(loss_penalized)\n",
    "\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "def make_encoder(hidden_nodes_list, activation_functions_list):\n",
    "\n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    encoder = models.Sequential()\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            encoder.add(layers.Dense(n_nodes,activation=act, input_shape=(input_dim,)))\n",
    "        else: \n",
    "            encoder.add(layers.Dense(n_nodes,activation=act))\n",
    "\n",
    "        latent_dim = hidden_nodes_list[-1]\n",
    "    return encoder , latent_dim\n",
    "\n",
    "def make_decoder(hidden_nodes_list, activation_functions_list, latent_dim):\n",
    "    if len(hidden_nodes_list)!= len(activation_functions_list):\n",
    "        raise ValueError(\"length of hidden nodes list should be equal length of activation_functions_list\")\n",
    "    nLayers = len(hidden_nodes_list)\n",
    "    decoder = models.Sequential()\n",
    "    for i in range(nLayers): \n",
    "\n",
    "        act = activation_functions_list[i]\n",
    "        n_nodes = hidden_nodes_list[i]\n",
    "        if i==0:\n",
    "            decoder.add(layers.Dense(n_nodes, activation=act, input_shape=(latent_dim,)))\n",
    "        else: \n",
    "            decoder.add(layers.Dense(n_nodes,activation=act))\n",
    "    return decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "\n",
    "x_train_sc = Scaler.fit_transform(x_train)\n",
    "x_val_sc   = Scaler.transform(x_val)\n",
    "x_test_sc  = Scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max x_train = 1.1827440440829455,  max x_train_sc = 5.443224686477889\n",
      " min x_train = 0.0,  min x_train_sc = -29.442531091994336\n",
      "\n",
      " max x_val = 1.1680716392704869,  max x_val_sc = 5.26183861330238\n",
      " min x_val = 0.0,  min x_val_sc = -28.50838955195364\n",
      "\n",
      " max x_test = 1.1755607741167227,  max x_test_sc = 5.444434349771324\n",
      " min x_test = 0.0,  min x_test_sc = -27.33959116555318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f' max x_train = {np.max(x_train)},  max x_train_sc = {np.max(x_train_sc)}')\n",
    "print(f' min x_train = {np.min(x_train)},  min x_train_sc = {np.min(x_train_sc)}')\n",
    "print()\n",
    "print(f' max x_val = {np.max(x_val)},  max x_val_sc = {np.max(x_val_sc)}')\n",
    "print(f' min x_val = {np.min(x_val)},  min x_val_sc = {np.min(x_val_sc)}')\n",
    "print()\n",
    "print(f' max x_test = {np.max(x_test)},  max x_test_sc = {np.max(x_test_sc)}')\n",
    "print(f' min x_test = {np.min(x_test)},  min x_test_sc = {np.min(x_test_sc)}')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the scaled version \n",
    "nr = 2; nc = 5\n",
    "fig, axs = plt.subplots(nr,nc, figsize = (20,10))\n",
    "ir = 0; ic = 0\n",
    "for i in range(10):\n",
    "    axs[ir,ic].plot(-x_train[i,:], label = 'unscaled', alpha = .5)\n",
    "#     axs[ir,ic].plot(x_train_sc[i,:], label = 'scaled',  alpha = .5)\n",
    "    axs[ir,ic].legend()\n",
    "    ic +=1 \n",
    "    if ic == nc:\n",
    "        ic=0\n",
    "        ir+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for activation sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 09:34:59.983762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-28 09:34:59.983798: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-28 09:34:59.983824: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c0704a-s11.ufhpc): /proc/driver/nvidia/version does not exist\n",
      "2021-11-28 09:34:59.985222: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-28 09:35:01.648988: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "51/51 [==============================] - 6s 101ms/step - loss: 0.9951 - mse: 0.9951 - val_loss: 1.0228 - val_mse: 1.0228\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 6s 108ms/step - loss: 0.9922 - mse: 0.9922 - val_loss: 1.0212 - val_mse: 1.0212\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9912 - mse: 0.9912 - val_loss: 1.0207 - val_mse: 1.0207\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9907 - mse: 0.9907 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9903 - mse: 0.9903 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 4s 88ms/step - loss: 0.9899 - mse: 0.9899 - val_loss: 1.0197 - val_mse: 1.0197\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 1.0194 - val_mse: 1.0194\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9892 - mse: 0.9892 - val_loss: 1.0192 - val_mse: 1.0192\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 5s 91ms/step - loss: 0.9889 - mse: 0.9889 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9886 - mse: 0.9886 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.9885 - mse: 0.9885 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9882 - mse: 0.9882 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9881 - mse: 0.9881 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9879 - mse: 0.9879 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9878 - mse: 0.9878 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9877 - mse: 0.9877 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9875 - mse: 0.9875 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9873 - mse: 0.9873 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 5s 101ms/step - loss: 0.9871 - mse: 0.9871 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9869 - mse: 0.9869 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9868 - mse: 0.9868 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9866 - mse: 0.9866 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9864 - mse: 0.9864 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 5s 102ms/step - loss: 0.9863 - mse: 0.9863 - val_loss: 1.0192 - val_mse: 1.0192\n",
      "Epoch 25/100\n",
      "51/51 [==============================] - 5s 101ms/step - loss: 0.9860 - mse: 0.9860 - val_loss: 1.0193 - val_mse: 1.0193\n",
      "Epoch 26/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9859 - mse: 0.9859 - val_loss: 1.0194 - val_mse: 1.0194\n",
      "0.005968016103785186\n",
      "training for activation tanh\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.9948 - mse: 0.9948 - val_loss: 1.0222 - val_mse: 1.0222\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 4s 87ms/step - loss: 0.9918 - mse: 0.9918 - val_loss: 1.0209 - val_mse: 1.0209\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 5s 89ms/step - loss: 0.9909 - mse: 0.9909 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 4s 85ms/step - loss: 0.9903 - mse: 0.9903 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 5s 90ms/step - loss: 0.9899 - mse: 0.9899 - val_loss: 1.0196 - val_mse: 1.0196\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 5s 90ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 1.0194 - val_mse: 1.0194\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.9892 - mse: 0.9892 - val_loss: 1.0192 - val_mse: 1.0192\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.9889 - mse: 0.9889 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 4s 88ms/step - loss: 0.9887 - mse: 0.9887 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9885 - mse: 0.9885 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9883 - mse: 0.9883 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9881 - mse: 0.9881 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.9880 - mse: 0.9880 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9878 - mse: 0.9878 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9876 - mse: 0.9876 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9875 - mse: 0.9875 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9874 - mse: 0.9874 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.9873 - mse: 0.9873 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9871 - mse: 0.9871 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9870 - mse: 0.9870 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.9868 - mse: 0.9868 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9866 - mse: 0.9866 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9865 - mse: 0.9865 - val_loss: 1.0192 - val_mse: 1.0192\n",
      "Epoch 24/100\n",
      "51/51 [==============================] - 5s 100ms/step - loss: 0.9863 - mse: 0.9863 - val_loss: 1.0193 - val_mse: 1.0193\n",
      "0.00597147476905179\n",
      "training for activation linear\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 6s 112ms/step - loss: 1.0031 - mse: 1.0031 - val_loss: 1.0284 - val_mse: 1.0284\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 5s 91ms/step - loss: 0.9982 - mse: 0.9982 - val_loss: 1.0240 - val_mse: 1.0240\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.9913 - mse: 0.9913 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9899 - mse: 0.9899 - val_loss: 1.0193 - val_mse: 1.0193\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.9893 - mse: 0.9893 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9889 - mse: 0.9889 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9887 - mse: 0.9887 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.9885 - mse: 0.9885 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9884 - mse: 0.9884 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9883 - mse: 0.9883 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9882 - mse: 0.9882 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 5s 100ms/step - loss: 0.9881 - mse: 0.9881 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9880 - mse: 0.9880 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9879 - mse: 0.9879 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 5s 101ms/step - loss: 0.9878 - mse: 0.9878 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9878 - mse: 0.9878 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 5s 100ms/step - loss: 0.9876 - mse: 0.9876 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 0.9876 - mse: 0.9876 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.9875 - mse: 0.9875 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9874 - mse: 0.9874 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 5s 100ms/step - loss: 0.9873 - mse: 0.9873 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 5s 101ms/step - loss: 0.9872 - mse: 0.9872 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "0.005982140132008806\n",
      "training for activation relu\n",
      "Epoch 1/100\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 0.9984 - mse: 0.9984 - val_loss: 1.0257 - val_mse: 1.0257\n",
      "Epoch 2/100\n",
      "51/51 [==============================] - 5s 89ms/step - loss: 0.9942 - mse: 0.9942 - val_loss: 1.0218 - val_mse: 1.0218\n",
      "Epoch 3/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9914 - mse: 0.9914 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 4/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9903 - mse: 0.9903 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "Epoch 5/100\n",
      "51/51 [==============================] - 5s 90ms/step - loss: 0.9898 - mse: 0.9898 - val_loss: 1.0195 - val_mse: 1.0195\n",
      "Epoch 6/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9894 - mse: 0.9894 - val_loss: 1.0193 - val_mse: 1.0193\n",
      "Epoch 7/100\n",
      "51/51 [==============================] - 4s 88ms/step - loss: 0.9890 - mse: 0.9890 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 8/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9888 - mse: 0.9888 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 9/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9885 - mse: 0.9885 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 10/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9883 - mse: 0.9883 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 11/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9881 - mse: 0.9881 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 12/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9879 - mse: 0.9879 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 13/100\n",
      "51/51 [==============================] - 5s 94ms/step - loss: 0.9878 - mse: 0.9878 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 14/100\n",
      "51/51 [==============================] - 5s 91ms/step - loss: 0.9876 - mse: 0.9876 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 15/100\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 0.9875 - mse: 0.9875 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 16/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9873 - mse: 0.9873 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 17/100\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 0.9872 - mse: 0.9872 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 18/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9870 - mse: 0.9870 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 19/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9868 - mse: 0.9868 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 20/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9867 - mse: 0.9867 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 21/100\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 0.9866 - mse: 0.9866 - val_loss: 1.0190 - val_mse: 1.0190\n",
      "Epoch 22/100\n",
      "51/51 [==============================] - 5s 102ms/step - loss: 0.9865 - mse: 0.9865 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 23/100\n",
      "51/51 [==============================] - 5s 93ms/step - loss: 0.9862 - mse: 0.9862 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "0.005977181386557488\n"
     ]
    }
   ],
   "source": [
    "# results_dict = defaultdict(lambda:'not present ')\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "\n",
    "hidden_nodes = [264,128,6]\n",
    "hidden_nodes_dec = [128,264,input_dim]\n",
    "\n",
    "activation_e1= [ 'linear','relu','linear']#,'tanh','tanh','tanh']\n",
    "\n",
    "activation_d1= [ 'relu','relu','relu']#,'tanh','tanh','tanh']\n",
    "\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "epochs = 100 \n",
    "batch_size = 128\n",
    "\n",
    "enc_act = ['sigmoid', 'tanh', 'linear','relu']\n",
    "\n",
    "for act in enc_act:\n",
    "    print(f'training for activation {act}')\n",
    "    activation_e1[1] = act\n",
    "    encoder,ld = make_encoder(hidden_nodes, activation_e1)\n",
    "    decoder = make_decoder(hidden_nodes_dec,activation_d1, latent_dim = ld)\n",
    "    network=models.Sequential()\n",
    "    network.add(encoder)\n",
    "    network.add(decoder)\n",
    "    network.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "### train the network \n",
    "    history = network.fit(x_train_sc,x_train_sc,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  callbacks=[earlystopping], # Early stopping\n",
    "                  validation_data=(x_val_sc,x_val_sc))\n",
    "#     network.save(filepath)\n",
    "\n",
    "## check reconstruction loss \n",
    "    test_set_latent_encoding = encoder(x_test_sc)\n",
    "    test_set_reconstructions = decoder(test_set_latent_encoding)\n",
    "        # \n",
    "    x_test_reconstructed = Scaler.inverse_transform(test_set_reconstructions)\n",
    "    rl = reconstr_loss(x_test, x_test_reconstructed, latent_dim = test_set_latent_encoding.shape[1])\n",
    "    print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = 2; nc = 5\n",
    "fig, axs = plt.subplots(nr,nc, figsize = (20,10))\n",
    "ir = 0; ic = 0\n",
    "for i in range(10):\n",
    "    axs[ir,ic].plot(x_test[i,:], label = 'data', alpha = .5)\n",
    "    axs[ir,ic].plot(x_test_reconstructed[i,:], label = 'recon',  alpha = .5)\n",
    "    ic +=1 \n",
    "    if ic == nc:\n",
    "        ic=0\n",
    "        ir+=1 \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(rl)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(x_test_sc)\n",
    "retransform = Scaler.inverse_transform(predictions)\n",
    "mse_loss = mean_squared_error(x_test, retransform, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
